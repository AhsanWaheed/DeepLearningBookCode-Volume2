{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "Copyright (c) 2017 Andrew Glassner\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "# Deep Learning From Basics to Practice\n",
    "## by Andrew Glassner, https://dlbasics.com, http://glassner.com\n",
    "------\n",
    "## Chapter 27: Applications\n",
    "### Notebook 4: Style Transfer\n",
    "\n",
    "This notebook is provided as a “behind-the-scenes” look at code used to make some of the figures in this chapter. It is still in the hacked-together form used to develop the figures, and is only lightly commented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run this code:\n",
    "- Find the cell marked Constants. \n",
    "- Set the variables there - in particular, set the input and output files\n",
    "- Save the notebook\n",
    "- Choose the Kernel menu, then Restart & Run All\n",
    "- Wait a while!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this code:\n",
    "This notebook is a minor restructuring of code from\n",
    "https://github.com/titu1994/Neural-Style-Transfer\n",
    "by Somshubra Majumdar (titu1994).\n",
    "\n",
    "See License E in LICENSE.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.misc import imread, imresize, imsave, fromimage, toimage\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Convolution2D, AveragePooling2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "\n",
    "# Just in case the Keras defaults aren't as we expect\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "def about():\n",
    "    \"\"\"\n",
    "    Neural Style Transfer with Keras 2.0.5\n",
    "\n",
    "    Based on:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/neural_style_transfer.py\n",
    "\n",
    "    Contains few improvements suggested in the paper Improving the Neural Algorithm of Artistic Style\n",
    "    (http://arxiv.org/abs/1605.04603).\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------------------\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_files = True\n",
    "\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0, os.path.dirname(current_dir)) # path to parent dir\n",
    "from DLBasics_Utilities import File_Helper\n",
    "file_helper = File_Helper(save_files)\n",
    "\n",
    "file_helper.check_for_directory(file_helper.get_saved_output_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "## Set up the transfer here, then reset and run the whole notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONSTANTS \n",
    "#\n",
    "# SET UP THE TRANFER HERE\n",
    "#\n",
    "base_image_path = 'input_data/waters-3038803_1280-crop.jpg'\n",
    "style_image_paths = [ 'input_data/HR-Self-Portrait-1907-Picasso.jpg' ]\n",
    "\n",
    "content_weight = 0.025\n",
    "style_weights = [1]\n",
    "image_size = 400\n",
    "total_variation_weight = 8.5e-5\n",
    "num_iter = 10\n",
    "\n",
    "model_name = 'vgg16'\n",
    "content_loss_type = 0\n",
    "rescale_image = True\n",
    "rescale_method = 'bicubic'\n",
    "maintain_aspect_ratio = True\n",
    "result_prefix = file_helper.get_saved_output_dir()+'/style-xfer-'\n",
    "\n",
    "content_layer = 'block1_conv2'\n",
    "num_style_layers = 13\n",
    "\n",
    "init_image ='content'  # try 'noise'\n",
    "pool_type_name = 'ave'\n",
    "preserve_color = False\n",
    "\n",
    "style_masks = None\n",
    "content_mask = None\n",
    "color_mask = None\n",
    "mask_path = None\n",
    "content_mask_path = None\n",
    "style_masks_present = False\n",
    "content_mask_present = False\n",
    "color_mask_present = False\n",
    "style_scale = 1.0\n",
    "min_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pooltype = 1 if pool_type_name == \"ave\" else 0\n",
    "read_mode = \"color\"\n",
    "\n",
    "# dimensions of the generated picture.\n",
    "img_width = img_height = 0\n",
    "img_WIDTH = img_HEIGHT = 0\n",
    "aspect_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "nb_tensors = None\n",
    "nb_style_images = None\n",
    "combination_image = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# util function to open, resize and format pictures into appropriate tensors\n",
    "def preprocess_image(image_path, load_dims=False, read_mode=\"color\"):\n",
    "    global img_width, img_height, img_WIDTH, img_HEIGHT, aspect_ratio\n",
    "\n",
    "    mode = \"RGB\" if read_mode == \"color\" else \"L\"\n",
    "    img = imread(image_path, mode=mode)  # Prevents crashes due to PNG images (ARGB)\n",
    "\n",
    "    if mode == \"L\":\n",
    "        # Expand the 1 channel grayscale to 3 channel grayscale image\n",
    "        temp = np.zeros(img.shape + (3,), dtype=np.uint8)\n",
    "        temp[:, :, 0] = img\n",
    "        temp[:, :, 1] = img.copy()\n",
    "        temp[:, :, 2] = img.copy()\n",
    "\n",
    "        img = temp\n",
    "\n",
    "    if load_dims:\n",
    "        img_WIDTH = img.shape[0]\n",
    "        img_HEIGHT = img.shape[1]\n",
    "        aspect_ratio = float(img_HEIGHT) / img_WIDTH\n",
    "\n",
    "        img_width = image_size\n",
    "        if maintain_aspect_ratio:\n",
    "            img_height = int(img_width * aspect_ratio)\n",
    "        else:\n",
    "            img_height = image_size\n",
    "\n",
    "    img = imresize(img, (img_width, img_height)).astype('float32')\n",
    "\n",
    "    # RGB -> BGR\n",
    "    img = img[:, :, ::-1]\n",
    "\n",
    "    img[:, :, 0] -= 103.939\n",
    "    img[:, :, 1] -= 116.779\n",
    "    img[:, :, 2] -= 123.68\n",
    "\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        img = img.transpose((2, 0, 1)).astype('float32')\n",
    "\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        x = x.reshape((3, img_width, img_height))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((img_width, img_height, 3))\n",
    "\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "\n",
    "    # BGR -> RGB\n",
    "    x = x[:, :, ::-1]\n",
    "\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "# util function to preserve image color\n",
    "def original_color_transform(content, generated, mask=None):\n",
    "    generated = fromimage(toimage(generated, mode='RGB'), mode='YCbCr')  # Convert to YCbCr color space\n",
    "\n",
    "    if mask is None:\n",
    "        generated[:, :, 1:] = content[:, :, 1:]  # Generated CbCr = Content CbCr\n",
    "    else:\n",
    "        width, height, channels = generated.shape\n",
    "\n",
    "        for i in range(width):\n",
    "            for j in range(height):\n",
    "                if mask[i, j] == 1:\n",
    "                    generated[i, j, 1:] = content[i, j, 1:]\n",
    "\n",
    "    generated = fromimage(toimage(generated, mode='YCbCr'), mode='RGB')  # Convert to RGB color space\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mask(mask_path, shape, return_mask_img=False):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        _, channels, width, height = shape\n",
    "    else:\n",
    "        _, width, height, channels = shape\n",
    "\n",
    "    mask = imread(mask_path, mode=\"L\") # Grayscale mask load\n",
    "    mask = imresize(mask, (width, height)).astype('float32')\n",
    "\n",
    "    # Perform binarization of mask\n",
    "    mask[mask <= 127] = 0\n",
    "    mask[mask > 128] = 255\n",
    "\n",
    "    max = np.amax(mask)\n",
    "    mask /= max\n",
    "\n",
    "    if return_mask_img: return mask\n",
    "\n",
    "    mask_shape = shape[1:]\n",
    "\n",
    "    mask_tensor = np.empty(mask_shape)\n",
    "\n",
    "    for i in range(channels):\n",
    "        if K.image_dim_ordering() == \"th\":\n",
    "            mask_tensor[i, :, :] = mask\n",
    "        else:\n",
    "            mask_tensor[:, :, i] = mask\n",
    "\n",
    "    return mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pooling_func(x):\n",
    "    if pooltype == 1:\n",
    "        return AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
    "    else:\n",
    "        return MaxPooling2D((2, 2), strides=(2, 2))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_tensor():\n",
    "    global nb_tensors, nb_style_images\n",
    "    global combination_image\n",
    "\n",
    "    # get tensor representations of our images\n",
    "    base_image = K.variable(preprocess_image(base_image_path, True, read_mode=read_mode))\n",
    "\n",
    "    style_reference_images = []\n",
    "    for style_path in style_image_paths:\n",
    "        style_reference_images.append(K.variable(preprocess_image(style_path)))\n",
    "\n",
    "    # this will contain our generated image\n",
    "    combination_image = K.placeholder((1, img_width, img_height, 3))\n",
    "\n",
    "    image_tensors = [base_image]\n",
    "    for style_image_tensor in style_reference_images:\n",
    "        image_tensors.append(style_image_tensor)\n",
    "    image_tensors.append(combination_image)\n",
    "\n",
    "    nb_tensors = len(image_tensors)\n",
    "    nb_style_images = nb_tensors - 2 # Content and Output image not considered\n",
    "\n",
    "    # combine the various images into a single Keras tensor\n",
    "    input_tensor = K.concatenate(image_tensors, axis=0)\n",
    "\n",
    "    shape = (nb_tensors, img_width, img_height, 3)\n",
    "\n",
    "    ip = Input(tensor=input_tensor, batch_shape=shape)\n",
    "    return ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model_and_feature_layers():\n",
    "    ip = get_input_tensor()\n",
    "    \n",
    "    # build the VGG16 network with our 3 images as input\n",
    "    x = Convolution2D(64, (3, 3), activation='relu', name='block1_conv1', padding='same')(ip)\n",
    "    x = Convolution2D(64, (3, 3), activation='relu', name='block1_conv2', padding='same')(x)\n",
    "    x = pooling_func(x)\n",
    "\n",
    "    x = Convolution2D(128, (3, 3), activation='relu', name='block2_conv1', padding='same')(x)\n",
    "    x = Convolution2D(128, (3, 3), activation='relu', name='block2_conv2', padding='same')(x)\n",
    "    x = pooling_func(x)\n",
    "\n",
    "    x = Convolution2D(256, (3, 3), activation='relu', name='block3_conv1', padding='same')(x)\n",
    "    x = Convolution2D(256, (3, 3), activation='relu', name='block3_conv2', padding='same')(x)\n",
    "    x = Convolution2D(256, (3, 3), activation='relu', name='block3_conv3', padding='same')(x)\n",
    "    if model_name == \"vgg19\":\n",
    "        x = Convolution2D(256, (3, 3), activation='relu', name='block3_conv4', padding='same')(x)\n",
    "    x = pooling_func(x)\n",
    "\n",
    "    x = Convolution2D(512, (3, 3), activation='relu', name='block4_conv1', padding='same')(x)\n",
    "    x = Convolution2D(512, (3, 3), activation='relu', name='block4_conv2', padding='same')(x)\n",
    "    x = Convolution2D(512, (3, 3), activation='relu', name='block4_conv3', padding='same')(x)\n",
    "    if model_name == \"vgg19\":\n",
    "        x = Convolution2D(512, (3, 3), activation='relu', name='block4_conv4', padding='same')(x)\n",
    "    x = pooling_func(x)\n",
    "\n",
    "    x = Convolution2D(512, (3, 3), activation='relu', name='block5_conv1', padding='same')(x)\n",
    "    x = Convolution2D(512, (3, 3), activation='relu', name='block5_conv2', padding='same')(x)\n",
    "    x = Convolution2D(512, (3, 3), activation='relu', name='block5_conv3', padding='same')(x)\n",
    "    if model_name == \"vgg19\":\n",
    "        x = Convolution2D(512, (3, 3), activation='relu', name='block5_conv4', padding='same')(x)\n",
    "    x = pooling_func(x)\n",
    "\n",
    "    model = Model(ip, x)\n",
    "    \n",
    "    TF_16_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    TF_19_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "    if model_name == \"vgg19\":\n",
    "        weights = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', TF_19_WEIGHTS_PATH_NO_TOP, cache_subdir='models')\n",
    "        feature_layers = ['block1_conv1', 'block1_conv2',\n",
    "                          'block2_conv1', 'block2_conv2',\n",
    "                          'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_conv4',\n",
    "                          'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_conv4',\n",
    "                          'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4']\n",
    "    else:\n",
    "        weights = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', TF_16_WEIGHTS_PATH_NO_TOP, cache_subdir='models')\n",
    "        feature_layers = ['block1_conv1', 'block1_conv2',\n",
    "                          'block2_conv1', 'block2_conv2',\n",
    "                          'block3_conv1', 'block3_conv2', 'block3_conv3',\n",
    "                          'block4_conv1', 'block4_conv2', 'block4_conv3',\n",
    "                          'block5_conv1', 'block5_conv2', 'block5_conv3' ]\n",
    "\n",
    "    model.load_weights(weights)\n",
    "\n",
    "    print('Model loaded.')\n",
    "    return (model, feature_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the neural style loss\n",
    "# first we need to define 4 util functions\n",
    "\n",
    "# Improvement 1\n",
    "# the gram matrix of an image tensor (feature-wise outer product) using shifted activations\n",
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        features = K.batch_flatten(x)\n",
    "    else:\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features - 1, K.transpose(features - 1))\n",
    "    return gram\n",
    "\n",
    "\n",
    "# the \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "def style_loss(style, combination, mask_path=None, nb_channels=None):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "\n",
    "    if content_mask_path is not None:\n",
    "        content_mask = K.variable(load_mask(content_mask_path, nb_channels))\n",
    "        combination = combination * K.stop_gradient(content_mask)\n",
    "        del content_mask\n",
    "\n",
    "    if mask_path is not None:\n",
    "        style_mask = K.variable(load_mask(mask_path, nb_channels))\n",
    "        style = style * K.stop_gradient(style_mask)\n",
    "        if content_mask_path is None:\n",
    "            combination = combination * K.stop_gradient(style_mask)\n",
    "        del style_mask\n",
    "\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_width * img_height\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "\n",
    "\n",
    "# an auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "def content_loss(base, combination):\n",
    "    channel_dim = 0 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    try:\n",
    "        channels = K.int_shape(base)[channel_dim]\n",
    "    except TypeError:\n",
    "        channels = K.shape(base)[channel_dim]\n",
    "    size = img_width * img_height\n",
    "\n",
    "    if content_loss_type == 1:\n",
    "        multiplier = 1. / (2. * (channels ** 0.5) * (size ** 0.5))\n",
    "    elif content_loss_type == 2:\n",
    "        multiplier = 1. / (channels * size)\n",
    "    else:\n",
    "        multiplier = 1.\n",
    "\n",
    "    return multiplier * K.sum(K.square(combination - base))\n",
    "\n",
    "\n",
    "# the 3rd loss function, total variation loss,\n",
    "# designed to keep the generated image locally coherent\n",
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    a = K.square(x[:, :img_width - 1, :img_height - 1, :] - x[:, 1:, :img_height - 1, :])\n",
    "    b = K.square(x[:, :img_width - 1, :img_height - 1, :] - x[:, :img_width - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_loss_and_grads(x):\n",
    "    x = x.reshape((1, img_width, img_height, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this Evaluator class makes it possible\n",
    "# to compute loss and gradients in one pass\n",
    "# while retrieving them via two separate functions,\n",
    "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# requires separate functions for loss and gradients,\n",
    "# but computing them separately would be inefficient.\n",
    "class Evaluator(object):\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model, feature_layers = get_model_and_feature_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "shape_dict = dict([(layer.name, layer.output_shape) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine the loss functions into a single scalar\n",
    "loss = K.variable(0.)\n",
    "layer_features = outputs_dict[content_layer]\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
    "#loss += content_weight * content_loss(base_image_features, combination_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Improvement 2\n",
    "# Use all layers for style feature extraction and reconstruction\n",
    "nb_layers = len(feature_layers) - 1\n",
    "\n",
    "style_masks = []\n",
    "if style_masks_present:\n",
    "    style_masks = mask_paths # If mask present, pass dictionary of masks to style loss\n",
    "else:\n",
    "    style_masks = [None for _ in range(nb_style_images)] # If masks not present, pass None to the style loss\n",
    "\n",
    "channel_index = 1 if K.image_dim_ordering() == \"th\" else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Improvement 3 : Chained Inference without blurring\n",
    "#AG print(\"len feature_layers = \",len(feature_layers))\n",
    "#print(\"summing up this many style layers: \",(min(args.num_style_layers, len(feature_layers)-1)))\n",
    "#print(\"nb_style_image = \",nb_style_images)\n",
    "layers_to_use  = min(num_style_layers, len(feature_layers))\n",
    "for i in range(layers_to_use-1):\n",
    "    layer_features = outputs_dict[feature_layers[i]]\n",
    "    shape = shape_dict[feature_layers[i]]\n",
    "    combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
    "    style_reference_features = layer_features[1:nb_tensors - 1, :, :, :]\n",
    "    sl1 = []\n",
    "    for j in range(nb_style_images):\n",
    "        #sl1.append(style_loss(style_reference_features[j], combination_features, style_masks[j], shape))\n",
    "        sl1.append(style_loss(style_reference_features[j], combination_features, None, shape))  # AG\n",
    "\n",
    "    layer_features = outputs_dict[feature_layers[i + 1]]\n",
    "    shape = shape_dict[feature_layers[i + 1]]\n",
    "    combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
    "    style_reference_features = layer_features[1:nb_tensors - 1, :, :, :]\n",
    "    sl2 = []\n",
    "    for j in range(nb_style_images):\n",
    "        # sl2.append(style_loss(style_reference_features[j], combination_features, style_masks[j], shape))\n",
    "        sl2.append(style_loss(style_reference_features[j], combination_features, None, shape)) # AG\n",
    "\n",
    "    for j in range(nb_style_images):\n",
    "        sl = sl1[j] - sl2[j]\n",
    "\n",
    "        # Improvement 4\n",
    "        # Geometric weighted scaling of style loss\n",
    "        loss += (style_weights[j] / (2 ** (layers_to_use- (i + 1)))) * sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "\n",
    "# get the gradients of the generated image wrt the loss\n",
    "grads = K.gradients(loss, combination_image)\n",
    "\n",
    "outputs = [loss]\n",
    "if type(grads) in {list, tuple}:\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "\n",
    "f_outputs = K.function([combination_image], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss\n",
    "\n",
    "\n",
    "if init_image == 'content':\n",
    "    x = preprocess_image(base_image_path, True, read_mode=read_mode)\n",
    "elif init_image == 'noise':\n",
    "    x = np.random.uniform(0, 255, (1, img_width, img_height, 3)) - 128.\n",
    "else:\n",
    "    print(\"Hey! Don't know init_image = \",init_image)\n",
    "\n",
    "# We require original image if we are to preserve color in YCbCr mode\n",
    "if preserve_color:\n",
    "    content = imread(base_image_path, mode=\"YCbCr\")\n",
    "    content = imresize(content, (img_width, img_height))\n",
    "\n",
    "    if color_mask_present:\n",
    "        color_mask_shape = (None, img_width, img_height, None)\n",
    "        color_mask = load_mask(color_mask, color_mask_shape, return_mask_img=True)\n",
    "    else:\n",
    "        color_mask = None\n",
    "else:\n",
    "    color_mask = None\n",
    "\n",
    "num_iter = num_iter\n",
    "prev_min_val = -1\n",
    "\n",
    "improvement_threshold = float(min_improvement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(num_iter):\n",
    "    print(\"Starting iteration %d of %d\" % ((i + 1), num_iter))\n",
    "    start_time = time.time()\n",
    "\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20)\n",
    "\n",
    "    if prev_min_val == -1:\n",
    "        prev_min_val = min_val\n",
    "\n",
    "    improvement = (prev_min_val - min_val) / prev_min_val * 100\n",
    "\n",
    "    print(\"Current loss value:\", min_val, \" Improvement : %0.3f\" % improvement, \"%\")\n",
    "    prev_min_val = min_val\n",
    "    # save current generated image\n",
    "    img = deprocess_image(x.copy())\n",
    "\n",
    "    if preserve_color and content is not None:\n",
    "        img = original_color_transform(content, img, mask=color_mask)\n",
    "\n",
    "    if not rescale_image:\n",
    "        img_ht = int(img_width * aspect_ratio)\n",
    "        print(\"Rescaling Image to (%d, %d)\" % (img_width, img_ht))\n",
    "        img = imresize(img, (img_width, img_ht), interp=rescale_method)\n",
    "\n",
    "    if rescale_image:\n",
    "        print(\"Rescaling Image to (%d, %d)\" % (img_WIDTH, img_HEIGHT))\n",
    "        img = imresize(img, (img_WIDTH, img_HEIGHT), interp=rescale_method)\n",
    "\n",
    "    fname = result_prefix + \"at_iteration_%d.png\" % (i + 1)\n",
    "    imsave(fname, img)\n",
    "    end_time = time.time()\n",
    "    print(\"Image saved as\", fname)\n",
    "    print(\"Iteration %d completed in %ds\" % (i + 1, end_time - start_time))\n",
    "\n",
    "    if improvement_threshold is not 0.0:\n",
    "        if improvement < improvement_threshold and improvement is not 0.0:\n",
    "            print(\"Improvement (%f) is less than improvement threshold (%f). Early stopping script.\" %\n",
    "                  (improvement, improvement_threshold))\n",
    "            exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
