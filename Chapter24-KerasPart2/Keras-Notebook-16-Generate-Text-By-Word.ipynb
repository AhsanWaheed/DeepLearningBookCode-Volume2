{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<small>\n",
    "Copyright (c) 2017 Andrew Glassner\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "# Deep Learning From Basics to Practice\n",
    "## by Andrew Glassner, https://dlbasics.com, http://glassner.com\n",
    "------\n",
    "## Chapter 23: Keras\n",
    "### Notebook 16: Generate text word by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Keras steps are a modified version of the character-based RNN at\n",
    "# https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "#\n",
    "# A lot of the word extraction and tokenizing was freely adapted from\n",
    "# http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "#\n",
    "# The Sherlock Holmes text is from Project Gutenberg\n",
    "# https://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import nltk.data\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a File_Helper for saving and loading files.\n",
    "\n",
    "save_files = True\n",
    "\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0, os.path.dirname(current_dir)) # path to parent dir\n",
    "from DLBasics_Utilities import File_Helper\n",
    "file_helper = File_Helper(save_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the stuff we need from the Natural Language Toolkit (NLTK)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "Vocabulary_size = 8000\n",
    "Batch_size = 64  # Set to 1 below if we're stateful\n",
    "Learning_rate = 0.01\n",
    "Num_epochs = 500\n",
    "Start_epoch = 1\n",
    "input_dir = file_helper.get_input_data_dir()\n",
    "Source_text_file = input_dir+'/holmes.txt'\n",
    "output_dir = file_helper.get_saved_output_dir()\n",
    "file_helper.check_for_directory(output_dir)\n",
    "Output_file = output_dir+'/generated-holmes.txt'\n",
    "\n",
    "Window_size = 40\n",
    "Window_step = 3\n",
    "Generated_text_length = 600\n",
    "Random_seed = 42\n",
    "Cells_per_layer = [8, 8]\n",
    "Use_dropout = [True] * len(Cells_per_layer)\n",
    "Dropout_rate = [0.3] * len(Cells_per_layer)\n",
    "Stateful_model = True  \n",
    "File_writer = None\n",
    "Model_name = 'Layers-'+str(Cells_per_layer)+'-stateful-'+str(Stateful_model)\n",
    "\n",
    "if Stateful_model:\n",
    "    Batch_size = 1             # so we can predict with just 1, probably better to modify predictions\n",
    "    Window_step = Window_size  # samples are sequential, not overlapping\n",
    "\n",
    "Unknown_token = \"GLORP\"  # all words not in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in text one sentence at a time: https://stackoverflow.com/questions/4576077/python-split-text-on-sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "fp = open(Source_text_file)\n",
    "data = fp.read()\n",
    "tokenized_sentences = tokenizer.tokenize(data)\n",
    "\n",
    "# remove punctuation https://stackoverflow.com/questions/23317458/how-to-remove-punctuation\n",
    "punctuations = [\n",
    "    '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', \n",
    "    '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', \n",
    "    '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', \n",
    "    '~', \"''\",\"`\",\"\\\"\", \",\", \"-\", \"\\n\", \"\\r\", \"‚Äù\"\n",
    "    ]\n",
    "sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    no_punc = \" \".join(\"\".join([\" \"+ch+\" \" if ch in punctuations else ch for ch in sentence]).split())\n",
    "    sentences.append(no_punc)\n",
    "    \n",
    "print(\"found \",len(sentences),\" sentences\")\n",
    "\n",
    "# sentences is an array of strings. Each string is what the tokenizer decided made\n",
    "# up an English-language \"sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_as_words = []\n",
    "for s in sentences:\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        text_as_words.append(w)\n",
    "print(\"the text contains \",len(text_as_words),\" words\")\n",
    "# text_as_words is all the words in the text after tokenizing and removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(text_as_words)\n",
    "number_of_unique_tokens = 1 + len(word_freq.items())  # add 1 for the \"unknown_token\"\n",
    "\n",
    "# Get the most common words \n",
    "vocab = word_freq.most_common(Vocabulary_size-1)\n",
    "print(\"Found \",len(vocab),\" distinct words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build index_to_word and word_to_index dictionaries\n",
    "unique_words = [v[0] for v in vocab]\n",
    "unique_words.append(Unknown_token)\n",
    "unique_words = sorted(list(set(unique_words)))\n",
    "print('number of unique vocabulary words being used:', len(unique_words))\n",
    "word_to_index = dict((w, i) for i, w in enumerate(unique_words))\n",
    "index_to_word = dict((i, w) for i, w in enumerate(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Using vocabulary size %d.' % Vocabulary_size)\n",
    "for i in range(10):\n",
    "    print(\"word popularity \"+str(i)+\": <\"+vocab[i][0]+\"> used \"+str(vocab[i][1])+\" times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i in range(len(text_as_words)):\n",
    "    if not text_as_words[i] in word_to_index:\n",
    "        text_as_words[i] = Unknown_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make huge list of windowed fragments\n",
    "fragments = []\n",
    "next_words = []\n",
    "for i in range(0, len(text_as_words) - Window_size, Window_step):\n",
    "    fragments.append(text_as_words[i: i + Window_size])\n",
    "    next_words.append(text_as_words[i + Window_size])\n",
    "print('number of fragments created:', len(fragments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clip the fragments so it's a multiple of the batch size\n",
    "keep_fragments = 64 * int(len(fragments)/64.)\n",
    "fragments = fragments[0:keep_fragments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "# X is a boolean array that is number-of-fragments * Window_size * vocabulary_size\n",
    "#    That is, every fragment contains Window_size entries, one for each word\n",
    "#    Each word is given by a one-hot encoding whose length is the total number of word tkens\n",
    "# y is a boolean array that is number-of-fragments * vocabulary_size\n",
    "#    Each entry is the one-hot encoding of the word that follows the corresponding fragment\n",
    "\n",
    "X = np.zeros((len(fragments), Window_size, Vocabulary_size), dtype=np.bool)\n",
    "y = np.zeros((len(fragments), Vocabulary_size), dtype=np.bool)\n",
    "for i, fragment in enumerate(fragments):\n",
    "    for t, word in enumerate(fragment):   \n",
    "        X[i, t, word_to_index[word]] = 1\n",
    "    y[i, word_to_index[next_words[i]]] = 1\n",
    "print(\"Training data:\")\n",
    "print(\"   X.shape = \",X.shape)\n",
    "print(\"   y.shape = \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    # layer 1 is special\n",
    "    if Stateful_model:\n",
    "        if Batch_size != 1:\n",
    "            print(\"*** WARNING! *** build_stateful_model: Batch_size should be 1\")\n",
    "        model.add(LSTM(Cells_per_layer[0], return_sequences=len(Cells_per_layer)>1,\n",
    "                           stateful=True,\n",
    "                           batch_input_shape=(1, Window_size, Vocabulary_size)))\n",
    "    else:\n",
    "        model.add(LSTM(Cells_per_layer[0], return_sequences=True,\n",
    "                       input_shape=(Window_size, Vocabulary_size)))\n",
    "    if Use_dropout[0]:\n",
    "        model.add(Dropout(Dropout_rate[0]))\n",
    "    for i in range(1, len(Cells_per_layer)):\n",
    "        return_sequence = i<len(Cells_per_layer)-1\n",
    "        model.add(LSTM(Cells_per_layer[i], return_sequences=return_sequence))\n",
    "        if Use_dropout:\n",
    "            model.add(Dropout(Dropout_rate[i]))\n",
    "    model.add(Dense(Vocabulary_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #optimizer = RMSprop(lr=Learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = preds[0:len(word_to_index)]\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_string(out_str=''):\n",
    "    print(out_str, end='')\n",
    "    File_writer.write(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_report():\n",
    "    print_string(\"Vocabulary_size = \"+str(Vocabulary_size)+\"\\n\")\n",
    "    print_string(\"Batch_size = \"+str(Batch_size)+\"\\n\")\n",
    "    print_string(\"Learning_rate = \"+str(Learning_rate)+\"\\n\")\n",
    "    print_string(\"Source_text_file = \"+str(Source_text_file)+\"\\n\")\n",
    "    print_string(\"Window_size = \"+str(Window_size)+\"\\n\")\n",
    "    print_string(\"Window_step = \"+str(Window_step)+\"\\n\")\n",
    "    print_string(\"Batch_size = \"+str(Batch_size)+\"\\n\")\n",
    "    print_string(\"Num_epochs = \"+str(Num_epochs)+\"\\n\")\n",
    "    print_string(\"Generated_text_length = \"+str(Generated_text_length)+\"\\n\\n\")\n",
    "\n",
    "    print_string(\"Input text file: \"+Source_text_file+'\\n')\n",
    "    print_string(\"    output file: \"+Output_file+'\\n\\n')\n",
    "    print_string(\"full text: \"+str(len(sentences))+\" sentences\\n\")\n",
    "    print_string(\"           \"+str(len(text_as_words))+\" tokens\\n\\n\")\n",
    "    print_string(\"           \"+str(number_of_unique_tokens)+\" unique tokens in source\\n\")\n",
    "    print_string(\"           \"+str(len(unique_words))+\" unique words (tokens) being used\\n\")\n",
    "    print_string('number of fragments created: '+str(len(fragments))+'\\n')\n",
    "    print_string('    resulting in '+str(len(fragments)/64.0)+' batches\\n\\n')\n",
    "    \n",
    "    print_string('Model_name: '+Model_name+'\\n')\n",
    "    print_string('Stateful_model: '+str(Stateful_model)+'\\n')\n",
    "    print_string('Cells per layer: '+str(Cells_per_layer)+'\\n')\n",
    "    print_string('Use dropout: '+str(Use_dropout)+'\\n')\n",
    "    print_string('Dropout rate: '+str(Dropout_rate)+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train the model, output generated text after each iteration\n",
    "# There needs to be a directory called \"Models\" in the same\n",
    "# directory as this file, or we'll get an error.\n",
    "\n",
    "File_writer = open(Output_file, 'w')\n",
    "print_report()\n",
    "model = build_model()\n",
    "Start_epoch = 1\n",
    "\n",
    "#### How to import from a saved model\n",
    "#import keras\n",
    "#model = keras.models.load_model('Models/Layers-[8, 8]-stateful-False-epoch-119.h5')\n",
    "#Start_epoch = 120\n",
    "\n",
    "shuffle = not Stateful_model\n",
    "\n",
    "np.random.seed(Random_seed)\n",
    "history_list = []\n",
    "\n",
    "for iteration in range(Start_epoch, Num_epochs):\n",
    "    print_string('\\n')\n",
    "    print_string('----------------------------------------------------------------------\\n')\n",
    "    print_string('Iteration '+str(iteration)+'\\n')\n",
    "    history = model.fit(X, y, Batch_size, epochs=1, shuffle=shuffle)  \n",
    "    history_list.append(history)\n",
    "    if Stateful_model:\n",
    "        model.reset_states()\n",
    "    print_string('Loss from iteration '+str(iteration)+' = '+str(history.history['loss'])+'\\n')\n",
    "        \n",
    "    model_filename = Model_name+'-epoch-'+str(iteration)\n",
    "    print(\"saving model to file \",model_filename)\n",
    "    file_helper.save_model(model, model_filename)  \n",
    "    start_index = random.randint(0, len(text_as_words) - Window_size - 1)\n",
    "\n",
    "    for diversity in np.linspace(.5, 2, 7):\n",
    "    #for diversity in [1]:\n",
    "        print_string('\\n')\n",
    "        print_string('----- diversity: '+str(diversity)+'\\n')\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text_as_words[start_index: start_index + Window_size]\n",
    "        #print(\"just made sentence =\",sentence)\n",
    "        generated = ' '.join(sentence)\n",
    "        print_string('----- Generating with seed: \"' +generated+ '\"\\n----\\n')\n",
    "        print_string(generated)\n",
    "\n",
    "        for i in range(Generated_text_length):\n",
    "            x = np.zeros((1, Window_size, Vocabulary_size))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x[0, t, word_to_index[word]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]            \n",
    "            \n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = index_to_word[next_index]\n",
    "\n",
    "            generated += ' '+next_word\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "            \n",
    "            print_string(' '+next_word)\n",
    "\n",
    "        print_string('\\n')\n",
    "        File_writer.flush()\n",
    "File_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
