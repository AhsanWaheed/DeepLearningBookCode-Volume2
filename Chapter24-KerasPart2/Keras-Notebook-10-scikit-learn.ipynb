{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<small>\n",
    "Copyright (c) 2017 Andrew Glassner\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "# Deep Learning From Basics to Practice\n",
    "## by Andrew Glassner, https://dlbasics.com, http://glassner.com\n",
    "------\n",
    "## Chapter 23: Keras\n",
    "### Notebook 10: Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as keras_backend\n",
    "keras_backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a File_Helper for saving and loading files.\n",
    "\n",
    "save_files = True\n",
    "\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0, os.path.dirname(current_dir)) # path to parent dir\n",
    "from DLBasics_Utilities import File_Helper\n",
    "file_helper = File_Helper(save_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# load the MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "image_height = X_train.shape[1]\n",
    "image_width = X_train.shape[2]\n",
    "number_of_pixels = image_height * image_width\n",
    "\n",
    "# cast the sample data to the current Keras floating-point type\n",
    "X_train = keras_backend.cast_to_floatx(X_train)\n",
    "X_test = keras_backend.cast_to_floatx(X_test)\n",
    "\n",
    "# scale data to range [0, 1]\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "# save y_train and y_test when used with scikit-learn\n",
    "original_y_train = y_train\n",
    "original_y_test = y_test\n",
    "\n",
    "# replace label data with one-hot encoded versions\n",
    "number_of_classes = 1 + max(np.append(y_train, y_test))\n",
    "y_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "\n",
    "# reshape to 2D grid, one line per image\n",
    "X_train = X_train.reshape(X_train.shape[0], number_of_pixels)\n",
    "X_test = X_test.reshape(X_test.shape[0], number_of_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a network of any number of (dense+ dropout) layers of the given size\n",
    "def make_model(number_of_layers=2, neurons_per_layer=32, dropout_ratio=0.2, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "\n",
    "    # first layer is special, because it sets input_shape\n",
    "    model.add(Dense(neurons_per_layer, input_shape=[number_of_pixels],\n",
    "        activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(dropout_ratio))\n",
    "    # now add in all the rest of the dense-dropout layers\n",
    "    for i in range(number_of_layers-1):\n",
    "        append_dense_dropout_layer(model, neurons_per_layer, dropout_ratio)\n",
    "    # wrap up with a softmax layer with 10 outputs\n",
    "    model.add(Dense(number_of_classes,  activation='softmax'))\n",
    "    # compile the model and return it\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def append_dense_dropout_layer(model, neurons_per_layer, dropout_ratio):\n",
    "    model.add(Dense(neurons_per_layer, \n",
    "        activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(dropout_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the KerasClassifier that packages up our model maker for CV and grid search\n",
    "#\n",
    "# Wrap up our model-making function into a KerasClassifier, which\n",
    "# will make it behave like a standard scikit-learn estimator.  We'll\n",
    "# give all the arguments defaults which we can override later when\n",
    "# we build the model as part of cross-validation or grid search.\n",
    "# For instance, we will provide a value to number_of_layers, so this\n",
    "# value will be passed to number_of_layers when make_model() is called.\n",
    "#\n",
    "\n",
    "kc_model = KerasClassifier(build_fn=make_model,\n",
    "                          number_of_layers=2, neurons_per_layer=32,\n",
    "                          nb_epoch=100, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_grid_search(param_grid, filename, verbose=2):\n",
    "# Run the first grid search\n",
    "    np.random.seed(random_seed)\n",
    "    grid_searcher = GridSearchCV(estimator=pipeline, param_grid=param_grid, verbose=verbose)\n",
    "    search_results = grid_searcher.fit(X_train, original_y_train)\n",
    "\n",
    "    print('---- GRID SEARCH ----')\n",
    "    print('mean test scores: {}\\n'.format(search_results.cv_results_['mean_test_score']))\n",
    "    best_index = np.argmax(search_results.cv_results_['mean_test_score'])\n",
    "    print('best set of parameters:\\n  index {}\\n  {}\\n'.format(best_index, \n",
    "                                                               search_results.cv_results_['params'][best_index]))\n",
    "\n",
    "    params = search_results.cv_results_['params']\n",
    "    dict_vals = [params[i].values() for i in range(len(params))]\n",
    "    name_list =[[str(v) for v in dv] for dv in dict_vals]\n",
    "    xlabels = ['-'.join(name_list[i]) for i in range(len(name_list))]\n",
    "    plt.plot(search_results.cv_results_['mean_test_score'], 'r')\n",
    "    plt.xticks(np.arange(len(xlabels)), xlabels, rotation='vertical')\n",
    "    file_helper.save_figure(filename+'-mean-test-scores')\n",
    "    plt.show()\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run CV on our data, without a normalization step (that is, no pipeline)\n",
    "np.random.seed(random_seed)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_seed)\n",
    "cv_results1 = cross_val_score(kc_model, X_train, original_y_train, cv=kfold, verbose=2)\n",
    "print(\"results1 ={}\\nresults1.mean={}\".format(cv_results1, cv_results1.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run a CV step with a pipeline\n",
    "np.random.seed(random_seed)\n",
    "pipeline = make_pipeline(MinMaxScaler(), kc_model)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_seed)\n",
    "cv_results2 = cross_val_score(pipeline, X_train, original_y_train, cv=kfold, verbose=2)\n",
    "print(\"results2 ={}\\nresults2.mean={}\".format(cv_results2, cv_results2.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a long-form, named pipeline for grid search steps\n",
    "estimators = []\n",
    "estimators.append(('normalize_step', MinMaxScaler()))\n",
    "estimators.append(('model_step', kc_model))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Run the first grid search\n",
    "param_grid1 = dict(model_step__number_of_layers=[ 2, 3, 4 ],\n",
    "                   model_step__neurons_per_layer=[ 20, 30, 40 ],\n",
    "                   model_step__optimizer=[ 'adam', 'adadelta' ]) \n",
    "search_results1 = run_grid_search(param_grid1, 'GridSearch1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_results1.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_index1 = np.argmax(search_results1.cv_results_['mean_test_score'])\n",
    "print('best set of parameters:\\n  index {}\\n  {}\\n'.format(\n",
    "              best_index1, search_results1.cv_results_['params'][best_index1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid2 = dict(model_step__number_of_layers=[ 1, 2],\n",
    "                  model_step__neurons_per_layer=[ 50, 80, 110, 140, 170 ]) \n",
    "search_results2 = run_grid_search(param_grid2, 'GridSearch2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid3 = dict(model_step__number_of_layers=[ 1, 2],\n",
    "                  model_step__neurons_per_layer=[ 180, 280, 380, 480, 580 ]) \n",
    "search_results3 = run_grid_search(param_grid3, 'GridSearch3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
